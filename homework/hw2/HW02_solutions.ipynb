{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets, model_selection\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate linear regression with gradient descent\n",
    "\n",
    "\n",
    "##### Question 1: Compute gradient of the MSE loss\n",
    "\n",
    "The MSE loss is defined as \n",
    "\n",
    "$$ L(w_0, w_1) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\Big( y_n - (w_0 + w_1 x_n) \\Big)^2$$\n",
    "\n",
    "Let $u_n(w_0, w_1) = y_n - (w_0 + w_1 x_n)$ \n",
    "\n",
    "Then $L(w_0, w_1) = \\frac{1}{N} \\sum_{n=0}^{N-1} u_n(w_0, w_1)^2$\n",
    "\n",
    "And $$\\frac{\\partial L}{\\partial w_{0|1}} = \\frac{1}{N} \\sum_{n=0}^{N-1} 2 u_n \\frac{ \\partial u_n}{\\partial w_{0|1}}$$\n",
    "\n",
    "With $$\\frac{ \\partial u_n}{\\partial w_{0}} = -1 $$\n",
    "And $$\\frac{ \\partial u_n}{\\partial w_{1}} = -x_n $$\n",
    "\n",
    "Therefore\n",
    "$$\\frac{ \\partial L}{\\partial w_{0}} = - \\frac{2}{N} \\sum_{n=0}^{N-1} u_n(w_0, w_1)^2$$\n",
    "And $$\\frac{ \\partial L}{\\partial w_{1}} = - \\frac{2}{N} \\sum_{n=0}^{N-1} x_n u_n(w_0, w_1)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the X and y values from unilinear.csv\n",
    "datapoints = np.genfromtxt('unilinear.csv', delimiter=',')\n",
    "X = datapoints[:,0]\n",
    "Y = datapoints[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def gradient_loss_function(X,Y,w):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the loss function defined in question 1\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    # partial derivative (of MSE) with respect to w0\n",
    "    der_wrt_w0 = -2/N * np.sum(Y-(w[0]+w[1]*X))\n",
    "    # partial derivative (of MSE) with respect to w1\n",
    "    der_wrt_w1 = -2/N * np.sum(X*(Y-(w[0]+w[1]*X)))\n",
    "    # make a gradient vector from the partial derivatives    \n",
    "    return (np.array([der_wrt_w0,der_wrt_w1]))\n",
    "    \n",
    "    \n",
    "def gradient_descent(\n",
    "    X, \n",
    "    Y,\n",
    "    weights=np.array([0, 0]),\n",
    "    learning_rate=0.1,\n",
    "    max_iter=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Iteratively updates the weights using the steepest descent method\n",
    "    \"\"\"\n",
    "    for i in range(max_iter):\n",
    "        # Steepest descent algorithm formula\n",
    "        weights = weights - learning_rate * gradient_loss_function(X,Y,weights)\n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gradient_descent(X,Y)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form_solution(X,Y):\n",
    "    \"\"\"\n",
    "    Computes the closed form solution of the loss minimization pb\n",
    "    \"\"\"\n",
    "    # Convert X and Y into 2D np array with one column\n",
    "    # (necessary for np.matmul and np.linalg.pinv)\n",
    "    x = X.reshape((-1, 1))\n",
    "    x = np.c_[np.ones(len(x)), x]\n",
    "    y = Y.reshape((-1, 1))\n",
    "    # pinv computes the pseudo-inverse of X\n",
    "    # matmul performs matrix multiplications.\n",
    "    return np.matmul(np.linalg.pinv(x), y)\n",
    "\n",
    "weights_exp = closed_form_solution(X,Y)\n",
    "print(weights_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [10**(-i) for i in range(1,6)]\n",
    "for learning_rate in learning_rates:\n",
    "    weights = gradient_descent(X,Y, learning_rate=learning_rate)\n",
    "    print(\"Learning rate = %.5f\" %learning_rate, \"Weights: \", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Logistic regression on two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()   # Load Iris dataset\n",
    "print(iris[\"feature_names\"])  # To know which column is \"petal width\"\n",
    "print(iris[\"target_names\"])   # To know which class is \"versicolor\"\n",
    "X = iris.data[:, 3]           # Store the 4th feature (Petal width)    \n",
    "Y = iris.target               # Store the labels\n",
    "\n",
    "def split_wtr_versicolor(Y):\n",
    "    \"\"\"\n",
    "    Divides the Iris dataset into two classes: Iris-Versicolor and not Iris-Versicolor\n",
    "    \"\"\"\n",
    "    # return 1 when the class is 2 (Versicolor) otherwise return 0 (not Versicolor)\n",
    "    y_bin = np.where(Y == 2, np.ones_like(Y, dtype=int), np.zeros_like(Y, dtype=int))\n",
    "    return y_bin\n",
    "\n",
    "Ybis = split_wtr_versicolor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666                    # Fix random seed for reproducibility\n",
    "# Shuffle and split the data into train and a concatenation of validation and test sets with a ratio of 0.7/0.3:\n",
    "X_train, X_val_test, Y_train, Y_val_test = model_selection.train_test_split(X, Ybis, \n",
    "                                                                            test_size=0.3, \n",
    "                                                                            shuffle=True, \n",
    "                                                                            random_state=seed)\n",
    "seed = 221\n",
    "# Shuffle and split the data into validation and test sets with a ratio of 0.5/0.5:\n",
    "X_val, X_test, Y_val, Y_test = model_selection.train_test_split(X_val_test, Y_val_test, \n",
    "                                                                test_size=0.5, \n",
    "                                                                shuffle=True, \n",
    "                                                                random_state=seed)                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Train Linear regression model\n",
    "# -------------------------------------\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train.reshape(-1, 1), Y_train.reshape(-1,1))\n",
    "\n",
    "# -------------------------------------\n",
    "# Train Logistic regression model\n",
    "# -------------------------------------\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train.reshape(-1, 1), Y_train)\n",
    "\n",
    "# -------------------------------------\n",
    "# Plot the data\n",
    "# -------------------------------------\n",
    "plt.figure(figsize=(10,10))\n",
    "cmap = ListedColormap(['red', 'darkcyan'])  # Classes' respective colors\n",
    "plt.scatter(X, Ybis, c=Ybis,\n",
    "            cmap=cmap,\n",
    "            label='Versicolor class')\n",
    "\n",
    "# -------------------------------------\n",
    "# Plot linear regression predictions\n",
    "# -------------------------------------\n",
    "# We will use this X_new instead of X_val for better plots (look more continuous)\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "plt.plot(X_new, linear_regression.predict(X_new), label=\"Linear regression\")\n",
    "\n",
    "# -------------------------------------\n",
    "# Plot Logistic regression predictions\n",
    "# -------------------------------------\n",
    "Y_proba = logit.predict_proba(X_new)\n",
    "plt.plot(X_new, Y_proba[:, 1], \"g--\", label=\"Logit: Iris-Versicolor Prob\")\n",
    "plt.plot(X_new, Y_proba[:, 0], \"b--\", label=\"Logit: not Iris-Versicolor Prob\")\n",
    "\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression on two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris[\"feature_names\"])  # To know which columns are \"petal length\" and \"petal width\"\n",
    "X = iris.data[:, :2]          # Store the 3rd (petal length) and 4th feature (Petal width)    \n",
    "Y = iris.target               # Store the labels\n",
    "\n",
    "seed = 666                    # Fix random seed for reproducibility\n",
    "# Shuffle and split the data into train and a concatenation of validation and test sets with a ratio of 0.7/0.3:\n",
    "X_train, X_val_test, Y_train, Y_val_test = model_selection.train_test_split(X, Y, \n",
    "                                                                            test_size=0.3, \n",
    "                                                                            shuffle=True, \n",
    "                                                                            random_state=seed)\n",
    "seed = 221\n",
    "# Shuffle and split the data into validation and test sets with a ratio of 0.5/0.5:\n",
    "X_val, X_test, Y_val, Y_test = model_selection.train_test_split(X_val_test, Y_val_test, \n",
    "                                                                test_size=0.5, \n",
    "                                                                shuffle=True, \n",
    "                                                                random_state=seed)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot parameters:\n",
    "# Light colors for decision boundaries plots:\n",
    "cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "cmap_bold = ListedColormap(['red', 'darkcyan', 'darkblue'])\n",
    "\n",
    "def plot_iris(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_val_test,\n",
    "    Y_val_test,\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter plots of training and testing iris datapoints\n",
    "\n",
    "    Colors represent specific iris species \n",
    "    Validation or test points appear in light colors\n",
    "    Training points appear in bold colors\n",
    "    \"\"\"\n",
    "    # Matplotlib method to get current axis\n",
    "    ax = plt.gca()    \n",
    "    # Scatter plot validation or testing points using light colors\n",
    "    ax.scatter(\n",
    "        X_val_test[:, 0], X_val_test[:, 1], c=Y_val_test,\n",
    "        cmap=cmap_light, edgecolor='k', s=20, zorder=2\n",
    "    )\n",
    "    # Overlay the training points in bold colors:\n",
    "    ax.scatter(\n",
    "        X_train[:, 0], X_train[:, 1],c=Y_train,\n",
    "        cmap=cmap_bold, edgecolor='k', s=20, zorder=2\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Petal length')\n",
    "    plt.ylabel('Petal width')\n",
    "    return ax\n",
    "\n",
    "def draw_boundaries(\n",
    "    log_reg_model, \n",
    "    h=0.02,  # Step size in the mesh\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw boundaries as decided by the trained model\n",
    "    \"\"\"\n",
    "    ax = plt.gca()\n",
    "    [xmin, xmax] = ax.get_xlim()\n",
    "    [ymin, ymax] = ax.get_ylim()\n",
    "    # Generate the axis associated to the first feature: \n",
    "    x_axis = np.arange(xmin, xmax, h)\n",
    "    # Generate the axis associated to the 2nd feature: \n",
    "    y_axis = np.arange(ymin, ymax, h)\n",
    "    # Generate a meshgrid (2D grid) from the 2 axis:\n",
    "    x_grid, y_grid = np.meshgrid(x_axis, y_axis)\n",
    "    # Vectorize the grids into column vectors:\n",
    "    x_grid_vectorized = x_grid.flatten()\n",
    "    x_grid_vectorized = np.expand_dims(x_grid_vectorized, axis=1)\n",
    "    y_grid_vectorized = y_grid.flatten()\n",
    "    y_grid_vectorized = np.expand_dims(y_grid_vectorized, axis=1)\n",
    "    # Concatenate the vectorized grids:\n",
    "    grid = np.concatenate((x_grid_vectorized, y_grid_vectorized),\n",
    "                                  axis=1)\n",
    "    # Now you can use 'grid' as data to classify by the knn \n",
    "\n",
    "    # Predict concatenated features to get the decision boundaries:\n",
    "    decision_boundaries = log_reg_model.predict(grid)\n",
    "\n",
    "    # Reshape the decision boundaries into a 2D matrix:\n",
    "    decision_boundaries = decision_boundaries.reshape(x_grid.shape)\n",
    "    plt.pcolormesh(x_grid, y_grid, decision_boundaries, cmap=cmap_light, zorder=1)\n",
    "    return ax\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train, Y_train)\n",
    "ax = plot_iris(X_train, Y_train, X_val, Y_val)\n",
    "ax = draw_boundaries(logit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('nglm-env': conda)",
   "language": "python",
   "name": "python37464bitnglmenvconda9bb31515f3e24106ad8f4c308c827f48"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
